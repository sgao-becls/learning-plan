## 1 什么是机器学习
- 机器学习是从历史**数据**中自动分析并得到**模型**, 并利用模型对未知数据进行**预测**

### 1.1 数据集的构成
- 特征值 (+ 标签值(目标值, label))
- 某些数据集也可以没有标签值(目标值, label)
- 每一行称为样本

### 1.2 算法分类
- 监督学习
  - 标签值(目标值, label): 类别(有限个离散值) => 分类问题
    - KNN, 贝叶斯分类, 决策树与随机森林, 逻辑回归
  - 标签值(目标值, label): 连续型数据 => 回归问题
    - 线性回归, 岭回归
- 无监督学习
  - 标签值(目标值, label): 无 => 无监督学习
    - 聚类: K-Means


### 1.3 开发流程
1. 获取数据
2. 数据处理 (pandas)
   - 缺失值处理
   - 降噪
3. 特征工程
   - 特征抽取
   - 数据预处理
   - 特征降维
4. 训练集和测试集划分
5. 机器学习算法训练 - 得到模型
6. 模型评估 (2 - 5 循环, 调优直到得到预期评估)
7. 预测(应用)

### 1.4 机器学习库和框架
- 传统机器学习框架
  -  sklearn
- 深度学习框架
  - TensorFlow
  - pytorch

### 1.5 书籍推荐(入门后阅读)
- 机器学习 (西瓜书) - 周志华
- 统计学习方法 - 李航
- 深度学习 (花书)


## 2. 特征工程
### 2.1 数据集
- 测试集和训练机
- sklearn

#### 2.1.1 可用数据集
- 学习可用
```
# UCI
https://archive.ics.uci.edu/ml/index.php
# kaggle
https://www.kaggle.com/datasets
# scikit-learn
https://scikit-learn.org/stable/datasets
```

#### 2.2.1.2 sklearn数据集
- sklean.**datasets**
    - datasets.load_*()    -  datasets.load_iris()
    - datasets.fetch_*()

- 数据集返回值, datasets.base.Bunch 继承字典
  - 键值对
    - data: 特征数据, 二维numpy.ndarray, n_samples * n_features
    - target: 标签数据(目标数据), 一维numpy.ndarray
    - DECR: 数据描述
    - feature_names: 特征名称
    - target_names: 标签名称
  - 获取方式 dict["key"] = values 或者 bunch.key = values

#### 2.1.3 数据集划分
- 训练数据: 用于训练,构建模型
- 测试数据: 评估模型
- 训练:测试 = 8:2 ~ 7:3

- sklearn划分数据集
  - sklearn.**model_selection**.train_test_split(arrays, *options)
    - x 数据集的特征值
    - y 数据集的标签值
    - test_size 测试集大小(float), 0.25 代表 25%的数据作为测试集
    - random_state 随机种子
    - return 训练集特征值, 测试集特征值, 训练集标签值, 测试集标签值
                x_train, x_test, y_train, y_test
```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)
```


### 2.2 特征工程
- 特征工程是使用专业背景知识和技巧处理数据, 使得特征能在机器学习算法上发挥更好的作用的过程. 
- 特征工程会影响机器学习的效果
- pandas 做数据清洗, 数据处理
- sklean 做特征工程

### 2.3特征抽取
- 机器学习算法 - 底层是统计学的方法 - 数学公式: 只能运算数值类型
  
#### 2.3.1 特征抽取
- 类型(字典) => 数值 
- 文本 => 数值
- 图像 => 数值
- sklearn.**feature_extraction**.*

#### 2.3.2 字典特征抽取
- 字典数据离散化 (onehot编码)
- sklearn.feature_extraction.**DictVectorizer**(sparse=True)
  - 父类 Transfer
  - DictVectorizer.fit_transform(X)
    - X: 字典
    - return: sparse矩阵
      - 稀疏矩阵: 将非零值, 按位置表示
  - DictVectorizer.inverse_transform(X)
    - X: ndarray, 或者sparse矩阵
    - return: 字典
  - DictVectorizer.get_feature_names() 返回类别名称
- 应用场景:
  - 类别特征 => 字典类型 => 字典特征抽取

#### 2.3.3 文本特征抽取
- 单词作为特征单位提取 - 特征词
- sklean.feature_extraction.text.**CountVectorizer**(stop_words=[])
  - 统计每个特征词出现的次数
  - CountVectorizer.fit_transform(X)
    - X: 文本或者包含文本的可以迭代对象
    - return: sparse矩阵
  - DictVectorizer.get_feature_names() 返回类别名称
  - stop_words 停用词表: 手动排出一些没有意义的特征词, 不计入统计

- 分词
  - 中文分词 jieba
  - list(jieba.cut(text))

- 关键词: 样本中出现次数有明显差异的特征词
  - TF-IDF: 表示一个词的重要程度
  - TF: (term frequency) 词频, 某个词在给定文章中的出现频率
  - IDF: inverse document frequency, 是一个词语普遍重要性的度量, 某一特定词语的idf,可以由**总文件数目除以包含该词语文件的数据, 再将得到的商取以10为底的对数得到**
- sklean.feature_extraction.text.**TfidfVectorizer**(stop_words=[])


### 2.4 特征预处理
- 无量纲化: 特征之间由于规格不统一, 产生了不同规格的数据, 无量纲化可以统一化这些数据, 使得这些数据可以在同一规格下进行比较
  - 归一化
  - 标准化
- sklearn.**preprocessing**.*

#### 2.4.1 归一化
- 将原始数据通过公式, 映射到一个指定区间内, 默认[0,1]
- 公式
  - X' = (x - min) / (max - min)
  - X“ = X‘ * (mx - mi) + mi
  - 作用于每一列, max为一列中的最大值, min为一列中的最小值. mx, mi分别为指定区间值, 默认mx为1, mi为0, [mi, mx]
- sklearn.preprocessing.**MinMaxScaler**(feature_range=(mi,mx)...)
  - MinMaxScaler.fix_transform(X)
    - X: ndarray
    - return: 相同形状的ndarry, 区间 [mi, mx]

- 缺陷: 受到异常值影响, 稳定性差

#### 2.4.2 标准化
- 将原始数据通过公式转换为均值为0, 标准差为1的范围内
- 公式
  - X‘ = (x - mean) / std
  - mean为平均值, std为标准差
  - 标准差反映一个数据集的离散程度
- sklearn.preprocessing.**StandardScaler**()
  - StandardScaler.fix_transform(X)
    - X: ndarray
    - return: 相同形状的ndarry

### 2.5 特征降维
- 降低特征的个数
- 得到一组“不相关”主变量的过程
  - 特征之间不相关

#### 2.5.1 特征选择
- sklearn.**feature_selection**.*
- 从原有特征中找出主要特征
  - filter
    - 方差选择法: 低方差特征过滤. 方差低表示分布集中, 说明比较相似
      - sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
        - VarianceThreshold.fit_transform(X)
          - X: ndarray
          - return: 低于threshold的特征将被删除, 默认值是保留所有非零方差特征, 即删除所有样本中具有相同值的特征
    - 相关系数
      - 取值范围[-1,1], 大于0正相关, 小于0负相关; 越接近于0, 相关性越小
      - 皮尔逊相关系数, 一个很大的公式
        - scipy.stats.**pearsonr** 
      - 特征选择, 如果特征相关性很高
        - 选择一个
        - 加权求和 - 新特征
        - 主成分分析

#### 2.5.2 主成分分析
- 高维数据转化为低维数据, 可能会舍弃原有数据,创造新的变量
- 作用: 是数据维数压缩, 尽可能降低原数据的维数(复杂度), 损失少量信息
- sklearn.decomposition.PCA(n_components=None)
  - 将数据分解为较低维数空间
  - n_components: 
    - 小数: 表示保留百分之多少的信息
    - 整数: 减少到多少特征
  - PCA.fit_transform(X)
    - X: ndarray
    - return: 降维后的ndarray


#### 2.5.3 t-SNE 
- 聚类算法?
- sklearn.manifold.TSNE(n_components=None)
  - n_components: 
    - 整数: 减少到多少特征, 画图的话就到2或3
  - TSNE.fit_transform(X)
    - X: ndarray
    - return: 降维后的ndarray








