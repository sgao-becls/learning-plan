## 1 什么是机器学习
- 机器学习是从历史**数据**中自动分析并得到**模型**, 并利用模型对未知数据进行**预测**

### 1.1 数据集的构成
- 特征值 (+ 标签值(目标值, label))
- 某些数据集也可以没有标签值(目标值, label)
- 每一行称为样本

### 1.2 算法分类
- 监督学习
  - 标签值(目标值, label): 类别(有限个离散值) => 分类问题
    - KNN, 贝叶斯分类, 决策树与随机森林, 逻辑回归
  - 标签值(目标值, label): 连续型数据 => 回归问题
    - 线性回归, 岭回归
- 无监督学习
  - 标签值(目标值, label): 无 => 无监督学习
    - 聚类: K-Means


### 1.3 开发流程
1. 获取数据
2. 数据处理 (pandas)
   - 缺失值处理
   - 降噪
3. 训练集和测试集划分
4. 特征工程
   - 特征抽取
   - 数据预处理
   - 特征降维
4. 机器学习算法训练 - 得到模型
   - 算法调优
     - 交叉验证
     - 网格搜索
5. 模型评估
6. 预测(应用)

### 1.4 机器学习库和框架
- 传统机器学习框架
  -  sklearn
     -  Transformer
        -  特征工程时调用的都是Transformer的子类, 用于统一调用流程
           -  实例化一个Transformer的子类
           -  调用实例的fit_transform()
              -  fit()   计算, 缓存transform时使用的必要参数, 如标准差, 降维参数, 缩放参数
              -  transform()   使用fit的结果进行转换,得到最后结果
     -  Estimator
        -  所有算法的实现都是Estimator的子类, 用于统一调用流程
           -  调用实例的fit_transform()
           -  fit(x_train, y_train)   训练模型
           -  y_predict = predict(x_test)     预测结果
              -  if y_predict == y_test
           -  score(x_test, y_test)       预测结果精度
- 深度学习框架
  - TensorFlow
  - pytorch

### 1.5 书籍推荐(入门后阅读)
- 机器学习 (西瓜书) - 周志华
- 统计学习方法 - 李航
- 深度学习 (花书)


## 2. 特征工程
### 2.1 数据集
- 测试集和训练机
- sklearn

#### 2.1.1 可用数据集
- 学习可用
```
# UCI
https://archive.ics.uci.edu/ml/index.php
# kaggle
https://www.kaggle.com/datasets
# scikit-learn
https://scikit-learn.org/stable/datasets
```

#### 2.2.1.2 sklearn数据集
- sklean.**datasets**
    - datasets.load_*()    -  datasets.load_iris()
    - datasets.fetch_*()

- 数据集返回值, datasets.base.Bunch 继承字典
  - 键值对
    - data: 特征数据, 二维numpy.ndarray, n_samples * n_features
    - target: 标签数据(目标数据), 一维numpy.ndarray
    - DECR: 数据描述
    - feature_names: 特征名称
    - target_names: 标签名称
  - 获取方式 dict["key"] = values 或者 bunch.key = values

#### 2.1.3 数据集划分
- 训练数据: 用于训练,构建模型
- 测试数据: 评估模型
- 训练:测试 = 8:2 ~ 7:3

- sklearn划分数据集
  - sklearn.**model_selection**.train_test_split(arrays, *options)
    - x 数据集的特征值
    - y 数据集的标签值
    - test_size 测试集大小(float), 0.25 代表 25%的数据作为测试集
    - random_state 随机种子
    - return 训练集特征值, 测试集特征值, 训练集标签值, 测试集标签值
                x_train, x_test, y_train, y_test
```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)
```


### 2.2 特征工程
- 特征工程是使用专业背景知识和技巧处理数据, 使得特征能在机器学习算法上发挥更好的作用的过程. 
- 特征工程会影响机器学习的效果
- pandas 做数据清洗, 数据处理
- sklean 做特征工程

### 2.3特征抽取
- 机器学习算法 - 底层是统计学的方法 - 数学公式: 只能运算数值类型
  
#### 2.3.1 特征抽取
- 类型(字典) => 数值 
- 文本 => 数值
- 图像 => 数值
- sklearn.**feature_extraction**.*

#### 2.3.2 字典特征抽取
- 字典数据离散化 (onehot编码)
- sklearn.feature_extraction.**DictVectorizer**(sparse=True)
  - 父类 Transfer
  - DictVectorizer.fit_transform(X)
    - X: 字典
    - return: sparse矩阵
      - 稀疏矩阵: 将非零值, 按位置表示
  - DictVectorizer.inverse_transform(X)
    - X: ndarray, 或者sparse矩阵
    - return: 字典
  - DictVectorizer.get_feature_names() 返回类别名称
- 应用场景:
  - 类别特征 => 字典类型 => 字典特征抽取

#### 2.3.3 文本特征抽取
- 单词作为特征单位提取 - 特征词
- sklean.feature_extraction.text.**CountVectorizer**(stop_words=[])
  - 统计每个特征词出现的次数
  - CountVectorizer.fit_transform(X)
    - X: 文本或者包含文本的可以迭代对象
    - return: sparse矩阵
  - DictVectorizer.get_feature_names() 返回类别名称
  - stop_words 停用词表: 手动排出一些没有意义的特征词, 不计入统计

- 分词
  - 中文分词 jieba
  - list(jieba.cut(text))

- 关键词: 样本中出现次数有明显差异的特征词
  - TF-IDF: 表示一个词的重要程度
  - TF: (term frequency) 词频, 某个词在给定文章中的出现频率
  - IDF: inverse document frequency, 是一个词语普遍重要性的度量, 某一特定词语的idf,可以由**总文件数目除以包含该词语文件的数据, 再将得到的商取以10为底的对数得到**
- sklean.feature_extraction.text.**TfidfVectorizer**(stop_words=[])


### 2.4 特征预处理
- 无量纲化: 特征之间由于规格不统一, 产生了不同规格的数据, 无量纲化可以统一化这些数据, 使得这些数据可以在同一规格下进行比较
  - 归一化
  - 标准化
- sklearn.**preprocessing**.*

#### 2.4.1 归一化
- 将原始数据通过公式, 映射到一个指定区间内, 默认[0,1]
- 公式
  - X' = (x - min) / (max - min)
  - X“ = X‘ * (mx - mi) + mi
  - 作用于每一列, max为一列中的最大值, min为一列中的最小值. mx, mi分别为指定区间值, 默认mx为1, mi为0, [mi, mx]
- sklearn.preprocessing.**MinMaxScaler**(feature_range=(mi,mx)...)
  - MinMaxScaler.fix_transform(X)
    - X: ndarray
    - return: 相同形状的ndarry, 区间 [mi, mx]

- 缺陷: 受到异常值影响, 稳定性差

#### 2.4.2 标准化
- 将原始数据通过公式转换为均值为0, 标准差为1的范围内
- 公式
  - X‘ = (x - mean) / std
  - mean为平均值, std为标准差
  - 标准差反映一个数据集的离散程度
- sklearn.preprocessing.**StandardScaler**()
  - StandardScaler.fix_transform(X)
    - X: ndarray
    - return: 相同形状的ndarry

### 2.5 特征降维
- 降低特征的个数
- 得到一组“不相关”主变量的过程
  - 特征之间不相关

#### 2.5.1 特征选择
- sklearn.**feature_selection**.*
- 从原有特征中找出主要特征
  - filter
    - 方差选择法: 低方差特征过滤. 方差低表示分布集中, 说明比较相似
      - sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
        - VarianceThreshold.fit_transform(X)
          - X: ndarray
          - return: 低于threshold的特征将被删除, 默认值是保留所有非零方差特征, 即删除所有样本中具有相同值的特征
    - 相关系数
      - 取值范围[-1,1], 大于0正相关, 小于0负相关; 越接近于0, 相关性越小
      - 皮尔逊相关系数, 一个很大的公式
        - scipy.stats.**pearsonr** 
      - 特征选择, 如果特征相关性很高
        - 选择一个
        - 加权求和 - 新特征
        - 主成分分析

#### 2.5.2 主成分分析
- 高维数据转化为低维数据, 可能会舍弃原有数据,创造新的变量
- 作用: 是数据维数压缩, 尽可能降低原数据的维数(复杂度), 损失少量信息
- sklearn.decomposition.PCA(n_components=None)
  - 将数据分解为较低维数空间
  - n_components: 
    - 小数: 表示保留百分之多少的信息
    - 整数: 减少到多少特征
  - PCA.fit_transform(X)
    - X: ndarray
    - return: 降维后的ndarray


#### 2.5.3 t-SNE 
- 聚类算法?
- sklearn.manifold.TSNE(n_components=None)
  - n_components: 
    - 整数: 减少到多少特征, 画图的话就到2或3
  - TSNE.fit_transform(X)
    - X: ndarray
    - return: 降维后的ndarray


## 3 分类算法

### 3.1 KNN算法
- sklearn.neighbers.KNeighbersClassifier(n_neighbers= x_train_size ** 0.5)
  - n_neighbers: K的取值, 选取的邻居数量
  - fit(x_train, y_train)
  - y_predict = predict(x_test)
  - score(x_test, y_test)


### 3.2 交叉验证和网格搜索
- 交叉验证: 将训练集数据分成n份, 循环n次, 每次迭代中, 取对应1/n的数据作为验证集, 其余数据作为训练集, 如此得到n份评估结果. 最终结果取n份结果的平均值
  - n 称为折数
- 网格搜索: 枚举算法的参数, 得到n个参数, 自动训练和评估n个参数所得到的对应结果
- sklearn.model_selection.**GridSearchCV**(estimator, param_grid=None, cv=None)
  - estimator: 算法评估器
  - param_grid: dict 比如knn中的k, {"n_neighbors:[1,3,5,7,9]}
  - cv: 交叉验证折数
  - GridSearchCV 也继承自Estimator
    - fit(x_train, y_train)
    - y_predict = predict(x_test)
    - score(x_test, y_test)

### 3.3 朴素贝叶斯算法


## 4 回归算法

### 4.1 线性回归
- 利用回归方程, 对多个自变量(特性值)和因变量(目标值)进行建模的过程
  - 通用公式: h(x)=w1x1 + w2x2 + w3x3 + ... + b = wTx + b
  - w: 权重, 回归系数
  - b: 偏置
- 线性模型
  - 自变量一次(线性关系)
    - 通用公式就是一个自变量一次的线性模型
  - 参数一次(可能是非线性关系)
    - y = w1x1 + w2x2^2 + w3x3^3 + w4x4^4 + ...  + b 也是线性模型
  - 线性关系一定是线性模型, 线性模型不一定是线性关系

#### 4.1.1 线性回归损失
- 目标: 求得模型参数(回归系数 + 偏置), 模型参数能够预测准确
- 损失函数: 描述预测结果和真实值的差异, 
  - 平方损失函数公式: f = (h(x1) - y1)^2 + (h(x1) - y1)^2 + ... + (h(xn) - yn)^2

#### 4.1.2 线性回归优化方法
- 梯度下降
- sklearn.linear_model.**SGDRegressor**(fit_intercept=True, loss="squared_loss", learning_rate="invscaling", eta0=0.01)
  - fit_intercept: 是否计算偏置
  - loss: 损失函数
  - learning_rate: 学习率方式
  - eta0: 初始学习率
  - max_iter: 最大迭代次数
  - SGDRegressor.fit(x_train, y_train)
    - SGDRegressor.coef_ : 回归系数
    - SGDRegressor.intercept_ : 偏置

#### 4.1.3 线性回归评估
- 均方误差
  - 越小越好
- slearn.metrics.**mean_squared_error**(y_real, y_predict)

#### 4.1.4 欠拟合和过拟合
- 欠拟合: 学习的数据, 特征太少
  - 增加数据的特征数量
- 过拟合: 特征过多, 并存在一些干扰特征
  - 正则化
    - L1正则化: 损失函数 + L1惩罚项 - Lasso回归
    - L2正则化: 损失函数 + L2惩罚项 - 岭回归 (常用)


#### 4.1.5 岭回归
- sklearn.linear_model.**Ridge**(alpha=1.0, fit_intercept=True, solver="auto", normalize=False)
  - alpha: 惩罚项系数 0-1 1-10
  - fit_intercept: 是否计算偏置
  - solver: 优化方法 SAG, SGD
  - normalize: 是否进行标准化
  - Ridge.fit(x_train, y_train)
    - Ridge.coef_ : 回归系数
    - Ridge.intercept_ : 偏置
- sklearn.linear_model.**RidgeCV**
  - 加入了交叉验证

### 4.2 逻辑回归
- 分类算法
- 线性回归的输出作为逻辑回归的输入
- 激活函数:
  - f(x) = 1 / (1 + e^(-x))
  - x = h(x) = w1x1 + w2x2 + w3x3 + ... + b = wTx + b  (变量x为回归函数)
  - f(x)值域(0,1)
  - 阈值设定, 比如 f(x)>0.8 => true
- 损失函数
  - 对数损失函数
- sklearn.linear_model.**LogisticRegression**(C=1.0, fit_intercept=True, penalty=“l2")
  - C: 惩罚项系数 0-1 1-10
  - penalty: 正则化函数
- 应用场景
  - 良性, 恶性
  - 次品, 优品

#### 4.2.1 评估逻辑回归: 精准度和召回率
- 精准率: 预测正例结果中, 真实正例的比例
- 召回率: 真正正例样本中, 预测结果为正例的比例
- sklearn.metrics.**classification_report**(y_real, y_predict, labels=[], target_names=[])
  - labels: 类别对应数字
  - target_names: 目标类别名称
